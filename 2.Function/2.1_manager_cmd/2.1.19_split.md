### 2.1.19  split 命令

### 背景
在进行POC时，现场人员进行数据导入时经常遇到各种问题，比较典型的是dble在导入文件时，对部分sql语句的不支持。另外，未分片的历史数据通过dble导入，旧数据会路由分片，在数据量较大时，耗时会比较长，在此过程中出现错误的话也很难排查。
基于以上原因，2.19.09.0版本提供工具对mysqldump导出的源数据文件按照后端分片节点进行分割。分割后的数据文件可以在每个后端分片执行导入，适配数据按分片导入的需求。

### dump文件语句处理
1. create database：会将逻辑数据库转换为物理库。
2. ddl语句：根据表的分片节点写入到对应后端节点的dump文件中，对于自增列，会将自增列的数据类型修改为bigint。
3. insert：全局序列列值会被dble替换为全局序列，再按照拆分列根据拆分算法路由到对应后端节点的dump文件中。
4. 一些属性设置的语句会根据最近一次解析的ddl来决定下发到具体的后端节点的dump文件中。
5. 会跳过对视图的处理
6. 会跳过对子表的处理

### 使用方法

#### 命令
在管理端口执行以下命令：
```bash
mysql > split src dest [-sschema] [-r500] [-w500] [-l10000]

- src：表示原始dump文件名
- dest：表示生成的dump文件存放的目录
- -s：表示默认逻辑数据库名，当dump文件中不包含schema的相关语句时，会默认导出到该schema
- -r：表示设置读文件队列大小，默认500
- -w：表示设置写文件队列大小，默认500
- -l：表示split后一条insert中最多包含的values，只针对分片表,默认4000
```
生成的分片文件以格式：源文件名-shardingNode名-时间戳.dump，最新的文件时间戳最大。

例如：我的原始dump文件是 /tmp/mysql_dump.sql ，我想切分以后输出到/tmp/dump/目录下：
命令就是：
```
split /tmp/mysql_dump.sql /tmp/dump/
```

#### 日志

默认情况下，split过程中生成的日志打印到在dble.log中，提供配置让split命令产生的日志单独存放，若需要开启，则需修改log4j.xml文件。

```xml
<Configuration status="WARN">
  <Appenders>
    <!-- 将下面的此段配置追加至已安装dble的log4j.xml中的Appenders下 -->
    <RollingFile name="DumpFileLog" fileName="logs/dump.log"
                  filePattern="logs/$${date:yyyy-MM}/dump-%d{MM-dd}-%i.log.gz">
      <PatternLayout>
        <Pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %5p [%t] (%l) - %m%n</Pattern>
      </PatternLayout>
      <Policies>
        <OnStartupTriggeringPolicy/>
        <SizeBasedTriggeringPolicy size="250 MB"/>
        <TimeBasedTriggeringPolicy/>
      </Policies>
      <DefaultRolloverStrategy max="10"/>
    </RollingFile>
  </Appenders>
  <Loggers>
    <!-- 将下面的此段配置追加至已安装dble的log4j.xml中的Loggers下，可通过调整level为debug来调整性能 -->
    <Logger name="dumpFileLog" level=“info” additivity="false" includeLocation="false" >
      <AppenderRef ref="DumpFileLog" />
      <AppenderRef ref="RollingFile"/>
    </Logger>
  </Loggers>
</Configuration>
```
可通过日志中的 “dump file has bean read d%”关键字来查看解析进度。

可开启日志的debug级别来调整性能

#### 任务停止

执行dump file任务的管理连接不受 idletimeout 参数的限制。用户可以通过kill @@connection id 方式杀掉管理连接从而停止dump file的任务的执行。

### 使用限制

1. 数据导入之后需要运维检查下数据完整性。
2. 对于使用全局序列的表，表原先全局序列中的值会被擦除，替换成全局序列，需要注意。
3. 暂时不支持子表的dump操作。
